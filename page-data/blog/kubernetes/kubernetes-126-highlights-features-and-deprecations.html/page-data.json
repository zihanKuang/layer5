{"componentChunkName":"component---src-templates-blog-single-js","path":"/blog/kubernetes/kubernetes-126-highlights-features-and-deprecations.html","result":{"data":{"mdx":{"body":"var _excluded = [\"components\"];\nfunction _extends() { return _extends = Object.assign ? Object.assign.bind() : function (n) { for (var e = 1; e < arguments.length; e++) { var t = arguments[e]; for (var r in t) ({}).hasOwnProperty.call(t, r) && (n[r] = t[r]); } return n; }, _extends.apply(null, arguments); }\nfunction _objectWithoutProperties(e, t) { if (null == e) return {}; var o, r, i = _objectWithoutPropertiesLoose(e, t); if (Object.getOwnPropertySymbols) { var n = Object.getOwnPropertySymbols(e); for (r = 0; r < n.length; r++) o = n[r], -1 === t.indexOf(o) && {}.propertyIsEnumerable.call(e, o) && (i[o] = e[o]); } return i; }\nfunction _objectWithoutPropertiesLoose(r, e) { if (null == r) return {}; var t = {}; for (var n in r) if ({}.hasOwnProperty.call(r, n)) { if (-1 !== e.indexOf(n)) continue; t[n] = r[n]; } return t; }\n/* @jsxRuntime classic */\n/* @jsx mdx */\n\nvar _frontmatter = {\n  \"title\": \"Kubernetes 1.26 Highlights, Features, and Deprecations\",\n  \"subtitle\": null,\n  \"date\": \"2022-12-06 08:00:00 -0530\",\n  \"author\": \"Lee Calcote\",\n  \"thumbnail\": \"./kubernetes-new.webp\",\n  \"darkthumbnail\": \"./kubernetes-new-dark.webp\",\n  \"description\": \"Release Notes: What changed in Kubernetes 1.26?\",\n  \"type\": \"Blog\",\n  \"category\": \"Kubernetes\",\n  \"tags\": [\"Kubernetes\"],\n  \"featured\": false,\n  \"published\": true\n};\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n    props = _objectWithoutProperties(_ref, _excluded);\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(BlogWrapper, {\n    mdxType: \"BlogWrapper\"\n  }, mdx(\"p\", null, \"As the final Kubernetes release of 2022, Kubernetes 1.26 is an exciting new release of the popular container orchestration platform. It offers a number of new features and improvements that will help platform engineers and DevOps engineers manage their Kubernetes clusters more effectively. Here are some of the highlights of this release.\"), mdx(\"div\", {\n    className: \"intro\"\n  }, mdx(\"p\", null, \"As a longstanding CNCF member, Layer5 has donated two of its open source projects to the CNCF: \", mdx(Link, {\n    to: \"/cloud-native-management/meshery\",\n    mdxType: \"Link\"\n  }, \"Meshery\"), \" and \", mdx(Link, {\n    to: \"/projects/cloud-native-performance\",\n    mdxType: \"Link\"\n  }, \"Service Mesh Performance\"), \". As an end-to-end, open-source, multi-cluster Kuberentes management platform, Meshery makes Day 2 Kubernetes cluster management a breeze. Run Meshery to explore the behavorial changes of this Kubernetes release and what they really mean to you.\")), mdx(\"p\", null, \"While there are a number of enhancments tracked in this release (38), you need to be aware that there are also a number of features being deprecated (10) in 1.26. In this article, we will focus on some highlighted enhancements, important deprecations, and removals so that you can be confident before upgrading your clusters. \"), mdx(\"p\", null, \"We'll breakdown new K8s features by category, starting with networking.\"), mdx(\"h2\", null, \"Networking in Kubernetes 1.26\"), mdx(\"h3\", null, mdx(\"a\", {\n    parentName: \"h3\",\n    \"href\": \"https://github.com/kubernetes/enhancements/issues/2086\"\n  }, \"Service Internal Traffic Policy\"), \" \", \"[Stable]\"), mdx(\"p\", null, \"When requests are made to a Kubernetes service, they are randomly distributed to all available endpoints. The new enhancement enriches the API of a service to use node-local and topology-aware routing for internal traffic. The new internalTrafficPolicy field has two options: Cluster (default) and Local. The Cluster option works like before and tries distributing requests to all available endpoints. On the other hand, the Local option only sends requests to node-local endpoints and drops the request if there is no available instance on the same node. The Local option is useful for sending metrics or logs to an agent running as a DaemonSet. \"), mdx(\"h3\", null, mdx(\"a\", {\n    parentName: \"h3\",\n    \"href\": \"https://github.com/kubernetes/enhancements/issues/3070\"\n  }, \"Reserve Service IP Ranges for Dynamic and Static IP Allocation\"), \" \", \"[Stable]\"), mdx(\"p\", null, \"Kubernetes services are assigned a virtual ClusterIP to be reachable inside the cluster. The ClusterIP is either assigned dynamically from a configured Service IP range, or statically set while creating the service resource. There was no possibility of knowing whether another service in the cluster had already used the static ClusterIP before this new stable enhancement. With this change, the IP range is divided into two; this prevents conflicts between services implementing dynamic IP allocation and static IP assignment. The flag --service-cluster-ip-range, with CIDR notation, is part of the Kubernetes API server configuration and is ready to use with the 1.26 release. \"), mdx(\"h3\", null, mdx(\"a\", {\n    parentName: \"h3\",\n    \"href\": \"https://github.com/kubernetes/enhancements/issues/1435\"\n  }, \"Support of Mixed Protocols in Services with Type LoadBalancer\"), \" \", \"[Stable]\"), mdx(\"p\", null, \"Kubernetes Services that use the LoadBalancer type have only supported a single Layer 4 protocol until now. With this enhancement going from graduating to stable in v1.26, it is possible to define a mix of protocols in the same service definition. In other words, this enhancement allows a LoadBalancer Service to serve different protocols (e.g. UDP, TCP) under the same port (e.g. 443). For example, serving both UDP and TCP requests for a DNS or SIP server on the same port. For instance, you can expose a DNS server with a single load balancer IP for both TCP and UDP requests, such as the following:\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-yaml\"\n  }, \"apiVersion: v1\\nkind: Service\\nmetadata:\\n  name: multi-protocol-dns-server\\nspec:\\n  type: LoadBalancer\\n  ports:\\n    - name: dns-udp\\n      port: 53\\n      protocol: UDP\\n    - name: dns-tcp\\n      port: 53\\n      protocol: TCP\\n  selector:\\n    app: dns-server\\n\")), mdx(\"h2\", null, \"Security in Kubernetes 1.26\"), mdx(\"h3\", null, mdx(\"a\", {\n    parentName: \"h3\",\n    \"href\": \"https://github.com/kubernetes/enhancements/issues/2133\"\n  }, \"kubelet Credential Provider\"), \" \", \"[Stable]\"), mdx(\"p\", null, \"The kubelet agent has a built-in credential provider mechanism to retrieve credentials for container image registries. It natively supports Azure, Google Cloud, and AWS container image registries for dynamically retrieving their credentials. The new stable enhancement in v1.26 offers a replacement for the in-tree implementations, and creates an API for extensible plugins in the future. \"), mdx(\"h3\", null, mdx(\"a\", {\n    parentName: \"h3\",\n    \"href\": \"https://github.com/kubernetes/enhancements/issues/3031\"\n  }, \"SignedSigning Release Artifacts\"), \" \", \"[Beta]\"), mdx(\"p\", null, \"Every Kubernetes release produces a set of artifacts such as binaries, container images, documentation, and metadata. Since the 1.24 release, the artifacts have been signed as an alpha feature. In the 1.26 release, artifact signing graduates to beta to increase software supply chain security for the Kubernetes release process and mitigate man-in-the-middle attacks.\"), mdx(\"h3\", null, mdx(\"a\", {\n    parentName: \"h3\",\n    \"href\": \"https://github.com/kubernetes/enhancements/issues/2799\"\n  }, \"Reduction of Secret-Based Service Account Tokens\"), \" \", \"[Beta]\"), mdx(\"p\", null, mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"BoundServiceAccountTokenVolume\"), \" has been GA since version 1.22: Service account tokens for pods are obtained via the TokenRequest API and stored as a projected volume. The new enhancement, in beta, eliminates the need to auto-generate secret-based service account tokens. In addition, Kubernetes will warn about using auto-created secret-based service account tokens, and purge the unused ones.\"), mdx(\"h3\", null, mdx(\"a\", {\n    parentName: \"h3\",\n    \"href\": \"https://github.com/kubernetes/enhancements/issues/1981\"\n  }, \"Windows Privileged Containers\"), \" \", \"[Stable]\", \" and \", mdx(\"a\", {\n    parentName: \"h3\",\n    \"href\": \"https://github.com/kubernetes/enhancements/issues/3503\"\n  }, \"Host Networking\"), \" \", \"[Alpha]\"), mdx(\"p\", null, \"Privileged containers are the ones that have similar access and capabilities to the host processes running on the servers. In Linux environments, they are used heavily in Kubernetes for storage, networking, and management. In this release, support for privileged containers for the Windows environment graduates to stable. Management of processes is heavily different from the operating system standpoint in Linux and Windows. Therefore, privileged containers will also work differently in two environments, but they will ensure the same level of security and operational experience.\"), mdx(\"p\", null, \"In addition, there is a new alpha-level enhancement in this release to support host networking for Windows pods. Currently, Windows has all the functionality to make containers use the networking namespace of the nodes. The new alpha enhancement enables this functionality from the Kubernetes side, increasing the parity between Linux and Windows containers.\"), mdx(\"h3\", null, mdx(\"a\", {\n    parentName: \"h3\",\n    \"href\": \"https://github.com/kubernetes/enhancements/issues/33250\"\n  }, \"Self-User Attribute and Authentication API\"), \" \", \"[Alpha]\"), mdx(\"p\", null, \"Kubernetes has no resources to identify and manage users as part of its API. Instead, it uses authenticators to get user attributes from tokens, certificates, OIDC providers, or webhooks. The new alpha feature adds a new API endpoint to see what attributes the current users have. The new API is under authentication.k8s.io with the name SelfSubjectReview, and there is a new corresponding command as well: kubectl auth who-am-i. The new feature will reduce the obscurity of complex authentication and help users debug the authentication stack. \"), mdx(\"h2\", null, \"Scheduling in Kubernetes 1.26\"), mdx(\"h3\", null, mdx(\"a\", {\n    parentName: \"h3\",\n    \"href\": \"https://github.com/kubernetes/enhancements/issues/2268\"\n  }, \"Non-Graceful Node Shutdown for StatefulSet Pods\"), \" \", \"[Beta]\"), mdx(\"p\", null, \"As a platform Kubernetes is hardened and has been deploy by thousands and thousands of users. Hardening of Kubernetes makes itself resistant to disasters. The kubelet agent that runs on each node in a Kubernetes cluster already uses graceful node shutdown to detect and offboard workloads to other nodes. However, when the shutdown is not detected by the kubelet, the pods of a \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"StatefulSet\"), \" are stuck as \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"Terminating\"), \" and not transferred to a healthy node. The kubelet on the downed node will not delete its pods from Kubernetes API, and the StatefulSet controller will not create new pods with the same name. This happens due to a conflict in the Kubernetes machinery. With this enhancement moving into beta, though, pods will be forcefully deleted along with their volume attachments and new pods will be migrated (created) on healthy nodes.\"), mdx(\"h3\", null, mdx(\"a\", {\n    parentName: \"h3\",\n    \"href\": \"https://github.com/kubernetes/enhancements/issues/3521\"\n  }, \"Pod Scheduling Readiness\"), \" \", \"[Alpha]\"), mdx(\"p\", null, \"Currently, pods are considered ready for scheduling as soon as they are created. However, not every pod requires a node, resource allocation, and the start of all its containers immediately after its creation. The new alpha enhancement adds an API to mark pods with their scheduling status: paused and ready. Pods with the .spec.schedulingGates field will be parked in the scheduler and only be assigned to nodes when they are ready to be scheduled.\"), mdx(\"h3\", null, mdx(\"a\", {\n    parentName: \"h3\",\n    \"href\": \"https://github.com/kubernetes/enhancements/issues/3515\"\n  }, \"kubectl explain to use OpenAPI v3 for \"), \" \", \"[Alpha]\"), mdx(\"p\", null, \"Use of OpenAPI v3 means supporting rich type information in \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"kubectl explan\"), \". Kubernetes has supported OpenAPI v3 as a beta since version 1.24. This richer representation of the fields in the Kubernetes API, means that users can use the \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"kubectl explain\"), \" command to get information that is only detailed in  OpenAPI v3, and not the subset defined OpenAPI v2.\"), mdx(\"h2\", null, \"Deprecations and Removals\"), mdx(\"p\", null, \"Consistent to the Kubernetes API lifecycle is deprecations and removals of APIs in each release. It is strongly suggested to check whether you are using the following APIs and flags before there are breaking changes.\"), mdx(\"ul\", null, mdx(\"li\", null, \"Removal of the `flowcontrol.apiserver.k8s.io/v1beta1` API group for `FlowSchema` and `PriorityLevelConfiguration` requires a migration to the v1beta2 API version.\"), mdx(\"li\", null, \"Removal of the `autoscaling/v2beta2` API version for HorizontalPodAutoscaler requires a migration to the autoscaling/v2 API version.\"), mdx(\"li\", null, \"Removal of legacy and vendor-specific authentication client-go and kubectl for Azure and Google Cloud requires migration to vendor-neutral authentication plugin mechanisms.\"), mdx(\"li\", null, \"Removal of in-tree CSI integration for OpenStack\\u2014namely, the `cinder` volume type\\u2014requires a migration to use the CSI driver for OpenStack.\"), mdx(\"li\", null, \"Some unused options and flags for the kubectl run command are marked as deprecated in the 1.26 release, such as `--grace-period`, `--timeout`, and `--wait`.\")), mdx(\"h2\", null, \"Last Kubernetes release of 2022\"), mdx(\"p\", null, \"Kubernetes is an ever-evolving platform. For those of you running workloads on Kubernetes taking detailed note of API changes and enhancements is an important activity as you endevour to keep your clusters upgraded with release releases. A more secure, scalable, and flexible Kubernetes is our collective goal. Dign into more details about deprecation, removals, and the latest changes in the 1.26 \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://relnotes.k8s.io/\"\n  }, \"release notes\"), \".\"), mdx(\"p\", null, \"On behalf of the Layer5 community and all of the CNCF projects that its contributors steward, thank you to everyone who participated in this Kubernetes release, and congratulations! \"), mdx(\"p\", null, \"As an end-to-end, open-source, multi-cluster Kuberentes management platform, Meshery makes Day 2 Kubernetes cluster management a breeze. Run Meshery to explore the behavorial changes of this Kubernetes release and what they really mean to you. \")));\n}\n;\nMDXContent.isMDXComponent = true;","frontmatter":{"title":"Kubernetes 1.26 Highlights, Features, and Deprecations","subtitle":null,"description":"Release Notes: What changed in Kubernetes 1.26?","date":"December 6th, 2022","author":"Lee Calcote","category":"Kubernetes","tags":["Kubernetes"],"thumbnail":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","placeholder":{"fallback":"data:image/webp;base64,UklGRpwAAABXRUJQVlA4IJAAAABQBACdASoUABQAPtFgqE+oJSOiKAgBABoJbADBzYrIcLaTwo9+PMnGQagAAP72Av+XnXpDovXZeeq4tTUzLytHh8EOuq1sofHN2IthfXcoC9jFvtZhVo+tDrkfaFYPTkFWBWNyondfXcOX/mOVgrxqRCV7kvjREw4wZJCrfL9qeXzh9e/A/AD1++Ek5WwAAAA="},"images":{"fallback":{"src":"/static/0cda4651daf491c6b40dd404799ca32c/5f169/kubernetes-new.webp","srcSet":"/static/0cda4651daf491c6b40dd404799ca32c/d66e1/kubernetes-new.webp 125w,\n/static/0cda4651daf491c6b40dd404799ca32c/e7160/kubernetes-new.webp 250w,\n/static/0cda4651daf491c6b40dd404799ca32c/5f169/kubernetes-new.webp 500w","sizes":"(min-width: 500px) 500px, 100vw"},"sources":[]},"width":500,"height":500}},"extension":"webp","publicURL":"/static/0cda4651daf491c6b40dd404799ca32c/kubernetes-new.webp"},"darkthumbnail":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","placeholder":{"fallback":"data:image/webp;base64,UklGRnYBAABXRUJQVlA4WAoAAAAQAAAAEwAAEwAAQUxQSLAAAAABgCMAAOHmYpuT+YPkBckH7C1pf9Cp3TzZ7WTb7mTbdie7xzgPiIgJAFI1z5uhikYcXPkDLwoMuEisW//g9w4I3uWRw+GHvq+P22vn3u6v1eGkh+B09c3seN3vnzmc8QPs2S8sHRr4AjPDpYMP1a1lDfVV279D+DDZ4N3kZX3bTfvj9xwrjGwB/O3oWO94AsHzvWcLBmC6ACO8pmMB/kjSgLDErLKysrKigMdjJ4ZLqVZQOCCgAAAAUAQAnQEqFAAUAD7RXqhPqCSjoigIAQAaCWwAsQWgJYz3ksiGQV2/pilkAADsw+6hq/cA70EZQnxQiDQur0nITK3Vk/X0a4mvrPL9VqKWuo91tPOP2mzL3NrOliPXZ4F6vkz5Oquym+BBBVS4AM9mRrF3dykeG8Q5yHw80ozivZetdD37I5//8WlnKn5Zb4/XAc7vyMlQ8r7aCdP07KxAAA=="},"images":{"fallback":{"src":"/static/4fe429ff2237fa91df4168525bb25419/5f169/kubernetes-new-dark.webp","srcSet":"/static/4fe429ff2237fa91df4168525bb25419/d66e1/kubernetes-new-dark.webp 125w,\n/static/4fe429ff2237fa91df4168525bb25419/e7160/kubernetes-new-dark.webp 250w,\n/static/4fe429ff2237fa91df4168525bb25419/5f169/kubernetes-new-dark.webp 500w","sizes":"(min-width: 500px) 500px, 100vw"},"sources":[]},"width":500,"height":500}},"extension":"webp","publicURL":"/static/4fe429ff2237fa91df4168525bb25419/kubernetes-new-dark.webp"}},"fields":{"slug":"/blog/kubernetes/kubernetes-126-highlights-features-and-deprecations"}}},"pageContext":{"slug":"/blog/kubernetes/kubernetes-126-highlights-features-and-deprecations"}},"staticQueryHashes":["112401468","1485533831","3750885592","4047814605","679004096"],"slicesMap":{},"matchPath":"/blog/kubernetes/kubernetes-126-highlights-features-and-deprecations"}